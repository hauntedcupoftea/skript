{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Skript.\n",
    "This project is the work of Anand Chauhan, Vasu Jain, and Aayush Gupta. Students enrolled at Bennett University, for the purpose of a Probability and Statistics Hackathon as part of their curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 22:44:30.142173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-05 22:44:30.142628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-05 22:44:30.142904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-05 22:44:30.143377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-05 22:44:30.143458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-05 22:44:30.143530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-05 22:44:30.143585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5703 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# importing all libraries in one cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening a sample speech, and seperate every sentence in the speech.\n",
    "f = open('sample.txt', 'r', encoding=\"utf8\")\n",
    "sample = np.array(re.split(r'(\\. )|(\\n)', f.read()))\n",
    "sample = sample[sample != np.array(None)]\n",
    "cleansample = np.array([i for i in sample if len(i) > 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy NLP model\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     POS    TAG    Dep    POS explained        tag explained \n",
      "Five     NUM    CD     nummod numeral              cardinal number\n",
      "score    NOUN   NN     npadvmod noun                 noun, singular or mass\n",
      "years    NOUN   NNS    npadvmod noun                 noun, plural\n",
      "ago      ADV    RB     advmod adverb               adverb\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "a        DET    DT     det    determiner           determiner\n",
      "great    ADJ    JJ     amod   adjective            adjective (English), other noun-modifier (Chinese)\n",
      "American ADJ    JJ     nsubj  adjective            adjective (English), other noun-modifier (Chinese)\n",
      "in       ADP    IN     prep   adposition           conjunction, subordinating or preposition\n",
      "whose    DET    WP$    poss   determiner           wh-pronoun, possessive\n",
      "symbolic ADJ    JJ     amod   adjective            adjective (English), other noun-modifier (Chinese)\n",
      "shadow   NOUN   NN     pobj   noun                 noun, singular or mass\n",
      "we       PRON   PRP    nsubj  pronoun              pronoun, personal\n",
      "stand    VERB   VBP    relcl  verb                 verb, non-3rd person singular present\n",
      "today    NOUN   NN     npadvmod noun                 noun, singular or mass\n",
      ",        PUNCT  ,      punct  punctuation          punctuation mark, comma\n",
      "signed   VERB   VBD    ROOT   verb                 verb, past tense\n",
      "the      DET    DT     det    determiner           determiner\n",
      "Emancipation PROPN  NNP    compound proper noun          noun, proper singular\n",
      "Proclamation PROPN  NNP    dobj   proper noun          noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "# a sample of tokenizing with Spacy\n",
    "doc = nlp(str(cleansample[1]))\n",
    "\n",
    "print(f\"{'text':{8}} {'POS':{6}} {'TAG':{6}} {'Dep':{6}} {'POS explained':{20}} {'tag explained'} \")\n",
    "for token in doc:\n",
    "    print(f'{token.text:{8}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{6}} {spacy.explain(token.pos_):{20}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tangent, making a numpy array with every individual word.\n",
    "words = [i.split() for i in cleansample]\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "cleanwords = []\n",
    "for i in words:\n",
    "    for x in i:\n",
    "        x.replace(punc, \"\")\n",
    "        cleanwords.append(x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count for shall is: 5\n",
      "count for will is: 27\n",
      "count for would is: 2\n",
      "count for must is: 8\n"
     ]
    }
   ],
   "source": [
    "# a simple script to detect persuasive sentences\n",
    "results = dict(Counter(cleanwords))\n",
    "modal = ['shall', 'should', 'will', 'would', 'must', 'ought', 'used', 'need', 'dare']\n",
    "for i in modal:\n",
    "    try:\n",
    "        print(f\"count for {i} is: {results[i]}\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing every word with the NLP model.\n",
    "rawspeech = \" \".join(cleansample.tolist())\n",
    "proc = nlp(rawspeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxcount = 0\n",
    "modcount = 0\n",
    "for i in proc:\n",
    "    if i.dep_ == 'aux':\n",
    "        auxcount += 1\n",
    "    if i.text in modal:\n",
    "        modcount += 1\n",
    "\n",
    "PERscore = modcount/auxcount\n",
    "PERscore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6314624a9b716fa5073b1b74923d7a0fe6a712a100f298bdf1d68454cdfed291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
